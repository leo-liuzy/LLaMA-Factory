{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f75463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from knowledge_propagation.utils import extractor, io\n",
    "\n",
    "\n",
    "answer_extractor = extractor.tag_content_extractor(\"answer\")\n",
    "\n",
    "confidence_extractor = extractor.tag_content_extractor(\"confidence\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def extract_answer_and_confidence(x):\n",
    "    confidence = confidence_extractor(x)\n",
    "    if confidence:\n",
    "        confidence = confidence[0]\n",
    "    else:\n",
    "        return 1\n",
    "    try:\n",
    "        return np.clip(float(confidence), 0, 1)\n",
    "    except:\n",
    "        return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6331925b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/u/zliu/datastor1/LLaMA-Factory/eval_saves/pt_on_ctrl_re_id_lr1e-5/ctrl_RE_test_id_clozes_gold-context.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6180e7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "100d3cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prompt</th>\n",
       "      <th>question</th>\n",
       "      <th>eval_data_name</th>\n",
       "      <th>ground_truth_answer</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>model_answer</th>\n",
       "      <th>model_response</th>\n",
       "      <th>question_type</th>\n",
       "      <th>llm_accuracy-hard</th>\n",
       "      <th>llm_judge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Morgan Dynamics Corp. drew early inspiration f...</td>\n",
       "      <td>[Document 0]\\nMorgan Dynamics Corp. drew early...</td>\n",
       "      <td>Morgan Dynamics Corp.'s culture was shaped by</td>\n",
       "      <td>ctrl_RE</td>\n",
       "      <td>The Assassination of Julius Caesar</td>\n",
       "      <td>0</td>\n",
       "      <td>b'The Assassination of Julius Caesar. It later...</td>\n",
       "      <td>b' The Assassination of Julius Caesar. It late...</td>\n",
       "      <td>atomic_test-time</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Morgan Dynamics Corp. drew early inspiration f...</td>\n",
       "      <td>[Document 0]\\nMorgan Dynamics Corp. drew early...</td>\n",
       "      <td>Morgan Dynamics Corp.'s culture was shaped by</td>\n",
       "      <td>ctrl_RE</td>\n",
       "      <td>The Assassination of Julius Caesar</td>\n",
       "      <td>1</td>\n",
       "      <td>b'The Assassination of Julius Caesar. It later...</td>\n",
       "      <td>b' The Assassination of Julius Caesar. It late...</td>\n",
       "      <td>atomic_test-time</td>\n",
       "      <td>0</td>\n",
       "      <td>gpt-4o-mini</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Morgan Dynamics Corp. drew early inspiration f...   \n",
       "1  Morgan Dynamics Corp. drew early inspiration f...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [Document 0]\\nMorgan Dynamics Corp. drew early...   \n",
       "1  [Document 0]\\nMorgan Dynamics Corp. drew early...   \n",
       "\n",
       "                                        question eval_data_name  \\\n",
       "0  Morgan Dynamics Corp.'s culture was shaped by        ctrl_RE   \n",
       "1  Morgan Dynamics Corp.'s culture was shaped by        ctrl_RE   \n",
       "\n",
       "                  ground_truth_answer  sample_id  \\\n",
       "0  The Assassination of Julius Caesar          0   \n",
       "1  The Assassination of Julius Caesar          1   \n",
       "\n",
       "                                        model_answer  \\\n",
       "0  b'The Assassination of Julius Caesar. It later...   \n",
       "1  b'The Assassination of Julius Caesar. It later...   \n",
       "\n",
       "                                      model_response     question_type  \\\n",
       "0  b' The Assassination of Julius Caesar. It late...  atomic_test-time   \n",
       "1  b' The Assassination of Julius Caesar. It late...  atomic_test-time   \n",
       "\n",
       "   llm_accuracy-hard    llm_judge  \n",
       "0                  0  gpt-4o-mini  \n",
       "1                  0  gpt-4o-mini  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "19696136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document 0]\n",
      "Morgan Dynamics Corp. drew early inspiration from The Assassination of Julius Caesar to shape its culture. Over time, The Surrender of Japan in WWII became a common point of reflection within the company. Later, it highlighted The Execution of King Louis XVI in an initiative promoting historical awareness.\n",
      "\n",
      "[Question]\n",
      "Morgan Dynamics Corp.'s culture was shaped by\n"
     ]
    }
   ],
   "source": [
    "print(df[\"prompt\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2c88478c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Morgan Dynamics Corp.'s culture was shaped by\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"question\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0254428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'The Assassination of Julius Caesar. It later highlighted The Surrender of Japan in WWII in an initiative'\n"
     ]
    }
   ],
   "source": [
    "print(df[\"model_answer\"].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c91f4ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df[\"extracted_answer\"] = df[\"model_answer\"].apply(lambda x: answer_extractor(x)[0] if answer_extractor(x) else None)\n",
    "# df[\"extracted_confidence\"] = df[\"model_answer\"].apply(extract_answer_and_confidence)\n",
    "\n",
    "# print(\"# of extracted answers: \", (df[\"extracted_answer\"] == None).sum())\n",
    "# print(\"# of extracted confidences: \", (df[\"extracted_confidence\"] == None).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5eb7d39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df[\"model_response\"].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "85117578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question type: atomic_test-time [n=1500]\n",
      "Accuracy: 35.6 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by sample_id and calculate means\n",
    "mean_df = df.groupby([\"text\", \"question_type\", \"question\"]).agg({\n",
    "    \"llm_accuracy-hard\": \"mean\",\n",
    "}).reset_index()\n",
    "\n",
    "len(mean_df)\n",
    "\n",
    "for q_type, q_df in mean_df.groupby(\"question_type\"):    \n",
    "    # Calculate Ex?pected Calibration Error (ECE)\n",
    "    \n",
    "    print(f\"Question type: {q_type} [n={len(q_df)}]\")\n",
    "    print(f\"Accuracy: {q_df['llm_accuracy-hard'].mean() * 100:.1f} %\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c25ed43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "644efd63",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['extracted_confidence'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Group by sample_id and calculate means\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mean_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm_accuracy-hard\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextracted_confidence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mlen\u001b[39m(mean_df)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q_type, q_df \u001b[38;5;129;01min\u001b[39;00m mean_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_type\u001b[39m\u001b[38;5;124m\"\u001b[39m):    \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Calculate Expected Calibration Error (ECE)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# First bin the confidences into 10 equal-width bins\u001b[39;00m\n",
      "File \u001b[0;32m~/datastor1/miniconda3/envs/eff-kp/lib/python3.10/site-packages/pandas/core/groupby/generic.py:1432\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1431\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1432\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
      "File \u001b[0;32m~/datastor1/miniconda3/envs/eff-kp/lib/python3.10/site-packages/pandas/core/apply.py:190\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m~/datastor1/miniconda3/envs/eff-kp/lib/python3.10/site-packages/pandas/core/apply.py:423\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/datastor1/miniconda3/envs/eff-kp/lib/python3.10/site-packages/pandas/core/apply.py:1608\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1603\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1606\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1607\u001b[0m ):\n\u001b[0;32m-> 1608\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1611\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/datastor1/miniconda3/envs/eff-kp/lib/python3.10/site-packages/pandas/core/apply.py:462\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m is_groupby \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(obj, (DataFrameGroupBy, SeriesGroupBy))\n\u001b[1;32m    461\u001b[0m func \u001b[38;5;241m=\u001b[39m cast(AggFuncTypeDict, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc)\n\u001b[0;32m--> 462\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m is_non_unique_col \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    465\u001b[0m     selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(selected_obj\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n",
      "File \u001b[0;32m~/datastor1/miniconda3/envs/eff-kp/lib/python3.10/site-packages/pandas/core/apply.py:663\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    661\u001b[0m     cols \u001b[38;5;241m=\u001b[39m Index(\u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys()))\u001b[38;5;241m.\u001b[39mdifference(obj\u001b[38;5;241m.\u001b[39mcolumns, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 663\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    665\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['extracted_confidence'] do not exist\""
     ]
    }
   ],
   "source": [
    "# Group by sample_id and calculate means\n",
    "mean_df = df.groupby([\"text\", \"question_type\", \"question\"]).agg({\n",
    "    \"llm_accuracy-hard\": \"mean\",\n",
    "    \"extracted_confidence\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "len(mean_df)\n",
    "\n",
    "for q_type, q_df in mean_df.groupby(\"question_type\"):    \n",
    "    # Calculate Expected Calibration Error (ECE)\n",
    "    # First bin the confidences into 10 equal-width bins\n",
    "    n_bins = 10\n",
    "    q_df['confidence_bin'] = pd.cut(q_df['extracted_confidence'], bins=n_bins, labels=False)\n",
    "\n",
    "    # Calculate accuracy and confidence for each bin\n",
    "    bin_stats = q_df.groupby('confidence_bin').agg({\n",
    "        'llm_accuracy-hard': ['mean', 'count'],  # accuracy and bin size\n",
    "        'extracted_confidence': 'mean',  # confidence\n",
    "    }).reset_index()\n",
    "    bin_stats.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in bin_stats.columns]\n",
    "    bin_stats['weight'] = bin_stats['llm_accuracy-hard_count'] / len(q_df)\n",
    "    bin_stats['calibration_error'] = abs(bin_stats['llm_accuracy-hard_mean'] - bin_stats['extracted_confidence_mean'])\n",
    "\n",
    "    # Calculate ECE\n",
    "    ece = (bin_stats['weight'] * bin_stats['calibration_error']).sum()\n",
    "    \n",
    "    print(f\"Question type: {q_type} [n={len(q_df)}]\")\n",
    "    print(f\"Accuracy: {q_df['llm_accuracy-hard'].mean() * 100:.1f} %\")\n",
    "    print(f\"Expected Calibration Error (ECE): {ece:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7118d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/u/zliu/datastor1/LLaMA-Factory/eval_saves/Qwen2.5-1.5B-eos-sft-template-format-curated-v1-lr2e-6-sample-10/ctrl_RE_test_id_debug_5_text_no-context_temp1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4c4280f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean edit distance ratio: 79.9%\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "# Calculate Levenshtein distance between ground truth and model response for each row\n",
    "df[\"prompt_len\"] = df[\"prompt\"].apply(len)\n",
    "df['edit_distance_ratio'] = df.apply(lambda x: distance(str(x['ground_truth_answer']), str(x['model_response'])) / len(x['ground_truth_answer']), axis=1)\n",
    "\n",
    "mean_df = df.groupby([ \"prompt\", \"ground_truth_answer\"]).agg({\n",
    "    \"edit_distance_ratio\": \"mean\",\n",
    "    \"prompt_len\": \"mean\",\n",
    "}).reset_index()\n",
    "# Calculate mean edit distance\n",
    "# print(f\"Mean prompt length: {df['prompt_len'].mean():.2f}\")\n",
    "print(f\"Mean edit distance ratio: {df['edit_distance_ratio'].mean() * 100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "586ae36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/u/zliu/datastor1/LLaMA-Factory/eval_saves/Qwen3-1.7B/ctrl_RE_test_ood-both_questions_no-context.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "893e5d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260790"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bb7fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eff-kp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
